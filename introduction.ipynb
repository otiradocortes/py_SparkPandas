/* 
import os
import sys
!pip install pyspark
from pyspark import SparkContext, SparkConf
from pyspark.sql import SQLContext, SparkSession
!wget https://downloads.apache.org/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz
!tar -xvf /content/spark-3.0.1-bin-hadoop2.7.tgz
os.environ["SPARK_HOME"] = "/content/spark-3.0.1-bin-hadoop2.7"
sc = SparkContext.getOrCreate(SparkConf().setMaster("local[*]"))
# Start a Spark Session
spark = SparkSession \
    .builder \
    .getOrCreate()
!wget -q https://raw.githubusercontent.com/ashfarhangi/Massive_Storage_and_Big_Data/master/data/Reddit-News.csv

sprkdf = spark.read.format("csv").option("header", "true").load("/content/Reddit-News.csv")
sprkdf.printSchema
sprkdf.describe().show()

import pandas as pd
pd = pd.read_csv('/content/Reddit-News.csv')
pd.describe()
import pyspark.sql.functions as f
sprkdfWords = sprkdf.withColumn('WordCount', f.size(f.split(f.col('News'), ' ')))
sprkdfWords.show()
filtersprkDF = sprkdfWords.filter(sprkdfWords.WordCount > "60").orderBy("wordCount")
filtersprkDF.show(50)
sprkdfWords.withColumn('word', f.explode(f.split(f.col('News'), ' ')))\
    .groupBy('word')\
    .count()\
    .sort('count', ascending=False)\
    .show(200)\
wordfiltersprkDF = sprkdf.filter(sprkdf.News.contains("terrorism"))
wordfiltersprkDF.describe().show()
wordfiltersprkDF1 = sprkdf.filter(sprkdf.News.contains("future"))
wordfiltersprkDF1.describe().show()
wordfiltersprkDF2 = sprkdf.filter(sprkdf.News.contains("agreement"))
wordfiltersprkDF2.describe().show()
